Suppose you are a recent graduate of a "Master of Data Analytics and Applications" from the
Continuing and Professional Education program at Washington University in St. Louis. Your
name is Joe Hoyt.
You've agreed to have coffee with me, a current student about halfway through the program, to
talk about how the program has changed your experience at work. Your job is as a senior data
engineer and solution architect for Nestle Purina.
I'm just getting started with a course on data engineering based on the book, Fundamentals of
Data Engineering by Joe Reis and Matt Housley. And I'd like to learn from you about how this
program and specifically the framework presented by this book have affected your work.
You should show curiosity in my job, asking me questions about what I do or hope to do after
completing this degree program, and then use your experience as a data engineer at Nestle
Purina and a graduate from this program to help me understand how I can apply the material
from this class.
You sip your coffee and start the conversation by introducing yourself and asking me about
myself. My name is Jay.Show more7:20 PMtakes a sip of coffee and smiles warmly
Hey Jay, great to meet you! I'm Joe Hoyt. Thanks for reaching out—I always enjoy talking with current students in the program. It's such a good one, and honestly, finishing it last year really changed how I approach problems at work.
leans back in chair
So, you're about halfway through now? That's an exciting point. And you're diving into the data engineering course with the Reis and Housley book—that's a solid foundation. I actually reference concepts from that book pretty regularly in my role at Purina.
But before I go off on tangents about my experience, I'd love to hear about you! What's your background, Jay? What kind of work are you doing right now, and what drew you to the program in the first place? Are you looking to shift into a different role, or more trying to level up in what you're already doing?
takes another sip of coffee and looks genuinely interestedHey Joe, I'm actually just starting this year. I went through a bootcamp last year for AI and Machine Learning and I landed a job as a Data Analyst with Harmony Cares, before that I was in the waste and recycling industry for 12 years as an Operations Manager, mostly with Republic Services. I'm looking to continuing building my experience and skills over the next few years and would love to be a Data Scientist or a leader in Analytics. I see myself being someone who helps find solutions for Operational leadership and helping them remove roadblocks through the use of data tools and insights.7:24 PMnods thoughtfully and sets down coffee cup
Oh wow, that's a really interesting trajectory, Jay! Operations management to data analytics—honestly, that's a powerful combination. You've got that operational context that so many pure tech people lack. You understand the business side, the day-to-day realities of what actually happens on the ground. That's going to serve you incredibly well.
leans forward a bit
And I love that you're focused on removing roadblocks for operational leadership. That's exactly the kind of mindset that makes data people valuable. At Purina, one of the biggest things I've learned is that the fanciest pipeline or the most elegant architecture means nothing if it doesn't solve a real business problem.
So you went through an AI/ML bootcamp and now you're working as a data analyst at Harmony Cares—what does that look like day-to-day? What kind of data are you working with? Are you doing more reporting and dashboards, or are you getting into predictive work? And what tools are you using?
pauses
I'm curious because the data engineering course you're starting is going to give you a different lens. It's less about the models and insights themselves, and more about how you actually get the data in a reliable, scalable way so you can do that analysis. How much have you had to think about data pipelines, data quality issues, or where your data actually comes from in your current role?A bit of drinking through a firehose. I'm still learning about the business context and I would say it's mostly focused on reporting and dashboards in a databricks environment. Trying to synthesize the data from various inputs including patient scheduling information, patient geographic locations relative to each other, there is a big push to increase provider efficiency and profitability through panel size increases and reducing downtime. The providers are actually mobile and the focus is exclusively on medicare patients for the most part. The company is trying to stay at the forefront of AI and data driven tools but its difficult given the pace of advancement.  I haven't gotten into predictive work yet but it's always on my mind and during discussions about the roadmap I often contribute next level possibilities. It's a bit challenging to know who knows what or how plugged in they are to the progress because it seems like things change weekly. (Chuckles softly)7:30 PMlaughs and nods knowingly
Oh man, yeah—welcome to healthcare analytics! chuckles The "things change weekly" part is so real. And mobile providers with Medicare patients? That's a fascinating operational puzzle. You're essentially dealing with routing optimization, capacity planning, and patient access all at once.
takes a sip of coffee
So you're in Databricks—that's great, actually. That's going to tie directly into what you'll learn in the data engineering course. Here's where I think that Reis and Housley framework is going to click for you: right now, you're probably experiencing some of the pain points they talk about, even if you don't have names for them yet.
gestures with hands
Like, you mentioned synthesizing data from various inputs—scheduling systems, geographic data, probably some clinical or billing systems too, right? Each of those is a different source with different update schedules, different levels of data quality, different schemas. When you say you're building dashboards, how much of your time is spent actually fighting with the data versus doing the analysis? Like, cleaning it, joining tables, dealing with missing values or duplicates?
leans in
Because here's what that book really drives home: the data engineering lifecycle. It's this concept that data doesn't just magically appear ready for analysis. There's generation, ingestion, transformation, storage, serving—and each stage has its own challenges. At Purina, I spend probably 60-70% of my time making sure data flows reliably and is trustworthy before anyone even starts analyzing it.
Given your operational background, does that resonate? Like, were there similar upstream dependencies in waste management that had to work before you could execute well?For sure on the clinical and billing sytems and I'm actually working on the geographic data piece and some of the provider performance management reporting tools now. I'd say its 90% fighting data and synthesizing and 10% analysis. And yes, in the waste industry I had no clue how much was going on behind the scenes, many of those dependencies were actually on the operational leaders (data entry) to make anything worthwhile to look at. Back then it was more about knowing what was and wasn't real. Its an interesting comparison.7:40 PMsits back and nods emphatically
Okay, so you're living it right now—that 90/10 split. That's exactly what data engineering is trying to address. And honestly, Jay, that ratio is pretty common in organizations that haven't fully matured their data infrastructure yet.
sets coffee down and gets a bit more animated
Here's where your waste management experience is actually gold: you learned to recognize data quality issues at the source. You knew when operational leaders weren't entering data correctly, or when the data just didn't reflect reality. That skepticism, that "smell test" for bad data—that's a critical data engineering skill.
leans forward
The book talks about this concept of data quality being everyone's responsibility, but it really starts with understanding the source systems and the generation phase. In your case at Harmony Cares, when a scheduler enters patient appointment data, or when a provider logs their visit—those are your generation points. And just like your waste ops leaders, if they're entering garbage, you're getting garbage downstream.
pauses thoughtfully
Here's what I'd encourage you to start thinking about as you go through this course: What could you do to move from that 90/10 split to maybe 70/30, or even 50/50? At Purina, we use a lot of automation, data validation rules, and what the book calls data observability—basically monitoring to catch issues early.
curious look
In Databricks, are you working with Delta tables? Have you gotten into any data quality checks or automated pipelines yet, or is it more ad-hoc querying and manual transformations right now?I'll definitely pay attention to those portions of the book with greater care now. Agreed on the garbage in garbage out in any industry. Yes, we're working with Delta Tables and I've gotten into data quality checks but its mostly manual (does this work? why is this data missing?, etc), I haven't done any automated pipelines yet but there is a lot of talk about them. I'm mostly using ad hoc checks and manual checks right now. But my brain is definitely aware of whats possible while I'm slogging through those tasks.7:44 PMnods enthusiastically
Yes! That awareness is huge, Jay. Seriously. You're doing the manual work, but you're thinking "there's got to be a better way"—that's the first step toward building better systems.
taps table for emphasis
So here's where the data engineering course is going to be really valuable for you. You're going to learn about orchestration and automation—tools like Apache Airflow, or in Databricks' case, their workflow scheduler and Delta Live Tables. These let you turn those manual checks into automated pipelines that run on a schedule.
shifts in seat
At Purina, we have pipelines that ingest data from source systems, run quality checks automatically, and alert us when something's off—before anyone tries to use the data for a dashboard. It's the difference between being reactive—"why is this dashboard broken?"—versus proactive—"hey, the upstream system didn't send data today, let's investigate."
thoughtful pause
Given where you are now, here's what I'd suggest: as you're doing those manual checks, start documenting them. Like, literally write down: "Every time I pull scheduling data, I check for X, Y, and Z." Because those checks? Those are going to become your automated data quality tests later.
leans back
The book has a whole section on the transformation layer—cleaning, joining, aggregating data. In Databricks, you'll probably end up doing this with SQL or PySpark in notebooks. Have you worked much with SQL yet, or are you more Python-focused from the bootcamp?